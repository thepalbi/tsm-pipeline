---
title: 23-12-2022
publish_date: 2022-12-23
---
La corrida de entrenamiento de tainted_path 100 termino. Fallaron algunas db, así que habría que revisar el log `/tesis/tainted_path_top_100.log`.

Ahora, hay que evaluar worse y v0:
```bash
# worse

python -m scripts.evaluate --codeql-source worse --dbs /tesis/repos/tsm-pipeline/experiments/tesis/tainted_path/tainted_path_100_split_test.txt --cache-root /tesis/dbs --output /tesis/tmp/results/tainted_path_top_100/worse --boost-results /tesis/tmp/results/tainted_path_top_100/averaged-results.csv

# v0
python -m scripts.evaluate --codeql-source v0 --dbs /tesis/repos/tsm-pipeline/experiments/tesis/tainted_path/tainted_path_100_split_test.txt --cache-root /tesis/dbs --output /tesis/tmp/results/tainted_path_top_100/v0
```

Finalmente estuve revisando la forma en que se computa las métricas, y los scores de recall, precision y accuracy son muyyy bajos:
```
$ python -m scripts.calculate_scores --results-dir=/tesis/tmp/results/tainted_path_top_100/
2022-12-23 23:59:39,684 - INFO - Result sets sizes: Worse 92, Boosted 34, V0 52, All 117
2022-12-23 23:59:39,687 - INFO - Score results: Precision: 0.0294. Recall: 0.0192. Accuracy: 0.2821
```

Creo que uno de los siguientes pasos sería generalizar todo el pipeline para las otras clases de dbs: `NoSQL`, `SQL` y `Xss`, y hacer una corrida como esta de top100 para cada una de ellas. Ya que voy a hacer esto, podría hacer un script en python que corra TODO! Creo que no sería tan dificil si la parte del orquestador utilizo una lib de docker. En ese caso, podría agregar a la lib de o11y que se guarden en la entidad `Experiment` los resultados.

Tomando la lista de dbs de atm que son de SQL, filtrando las borradas y tomando como mínimo 36% de líneas de Javascript, quedan 60 (/tesis/repos/tsm-pipeline/experiments/tesis/atm_sql.txt).

Haciendo los splits para lanzar unas corridas.
```
python -m scripts.split --name atm_sql --entries-file ../experiments/tesis/atm_sql.txt --train-size .7
```
from typing import Tuple
from collections import defaultdict
from sklearn.metrics import accuracy_score, precision_score, recall_score
from typing import Iterable, Set, Tuple
import pandas as pd
import glob
import os
import re
import logging
"""tesis scores.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xrk9z0PcyIPlod4aqLG8_GiP69XTLc4r
"""

log = logging.getLogger(__name__)


def _hash_tuple(k: str, x: Tuple) -> str:
    """hash_tuple creates a consistent hash for a given evaluations results row.

    :param str k: the name of the db from whom the results belong
    :param Tuple x: the row
    :return str: the hash
    """
    return "%s#" % (k)+"#".join([
        str(v) for v in x
    ])


def _project_name_from_hash(h: str) -> str:
    """_project_name_from_hash gets the project name from a hashed evaluation results row

    :param str h: the hash
    :return str: the extracted project name
    """
    i = h.index("#")
    return h[:i]


def hash_set_to_df(hs: Set[str]) -> pd.DataFrame:
    rows = []
    for h in hs:
        rows.append(h.split("#"))
    return pd.DataFrame(data=rows)


def _calculate_score_sets(
    results_folder: str,
    cleanup_base_dir: str,
    worse_dir: str,
    boost_dir: str,
    v0_dir: str,
) -> Tuple[Set[str], Set[str], Set[str]]:
    """_calculate_score_sets calculates the results sets that allow one to calculate scores.

    :param str results_folder: results folder for the experiment
    :param str cleanup_base_dir: directory where dbs were generated on, defaults to "/tesis/tmp"
    :param str boost_dir: dir inside results folder where worse evaluation results are saved, defaults to "worse"
    :param str boost_dir: dir inside results folder where boosted evaluation results are saved, defaults to "boosted"
    :param str v0_dir: dir inside results folder where v0 evaluation results are saved, defaults to "v0"
    :return Tuple[Set[str], Set[str], Set[str]]: v0, worse, boosted sets
    """

    results_worse = {}
    results_boost = {}
    results_v0 = {}

    for f in glob.glob(os.path.join(results_folder, worse_dir, "*.csv")):
        results_worse[os.path.basename(f)] = pd.read_csv(f)

    for f in glob.glob(os.path.join(results_folder, boost_dir, "*.csv")):
        results_boost[os.path.basename(f)] = pd.read_csv(f)

    for f in glob.glob(os.path.join(results_folder, v0_dir, "*.csv")):
        results_v0[os.path.basename(f)] = pd.read_csv(f)

    log.debug("Read total %s dataframes" %
              (len(results_worse)+len(results_boost) + len(results_v0)))

    # Post-processing required for results:
    # - remove the leading /tesis/tmp/*/ from the filePathSource and filePathSink columns
    # - drop score

    # Post-processing results
    # Clean filePath* columns
    def cleanup_filePath_col(value: str) -> str:
        # /tesis/tmp/78fi2yky
        leading_replace_pat = "%s/[a-zA-Z0-9]+/" % (cleanup_base_dir)
        return re.sub(leading_replace_pat, "", value)

    # This bit below will cleanup the /tesis/tmp/[a-zA-Z0-9]+ bit of the path column in the results
    for df in results_worse.values():
        # Cleanup filePath* columns
        df['filePathSource'] = df['filePathSource'].map(cleanup_filePath_col)
        df['filePathSink'] = df['filePathSink'].map(cleanup_filePath_col)

    for df in results_boost.values():
        # Cleanup filePath* columns
        df['filePathSource'] = df['filePathSource'].map(cleanup_filePath_col)
        df['filePathSink'] = df['filePathSink'].map(cleanup_filePath_col)

    for df in results_v0.values():
        df['filePathSource'] = df['filePathSource'].map(cleanup_filePath_col)
        df['filePathSink'] = df['filePathSink'].map(cleanup_filePath_col)

    # Now, generate the a set for worse, v0 and boosted alone
    v0 = set()
    worse = set()
    boosted = set()

    # cleanup of each dataframe, dropping columns that will not give repeatable results between
    # evaluation sets.
    def cleanup(df):
        return df.drop(["score", "origin", "source", "sink"], axis=1)

    # Calculate v0 set
    for k, df in results_v0.items():
        # Drop non hashed columns
        df = cleanup(df)
        # hash and add to set
        v0 = v0 | set(df.apply(lambda x: _hash_tuple(k, tuple(x)), axis=1))

    # Calculate boosted
    for k, df in results_boost.items():
        # filter out boosted
        df = cleanup(df)
        boosted = boosted | set(
            df.apply(lambda x: _hash_tuple(k, tuple(x)), axis=1))

    # Calculate worse
    for k, df in results_worse.items():
        # filter out boosted
        df = cleanup(df)
        worse = worse | set(
            df.apply(lambda x: _hash_tuple(k, tuple(x)), axis=1))

    # now since worse and boosted run in separate queries, I have to substract worse from boosted since they might have intersection
    return v0, worse, boosted-worse


def calculate_scores(
    results_folder: str,
    cleanup_base_dir="/tesis/tmp",
    use_v0_prime=True,
    worse_dir="worse",
    boost_dir="boosted",
    v0_dir="v0",
) -> Tuple[float, float, float]:
    """calculate_scores calcultes precision, recall and accuracy for the given experiment.

    :param str results_folder: the folder containing the results of the experiment
    :param str cleanup_base_dir: base dir to cleanup in source files from query results, defaults to "/tesis/tmp"
    :param bool use_v0_prime: whether or not to substract worse from v0 to caculate true results, defaults to True
    :return Tuple[float, float, float]: precision, recall and accuracy
    """
    v0, worse, boosted = _calculate_score_sets(
        results_folder, cleanup_base_dir, worse_dir, boost_dir, v0_dir)

    # Using instead of the whole set just the following sets:
    # - V0 prime, which is V0 - Worse
    # - Boosted, which is in the worseboosted query, just the results corresponding to boosted
    # by doing doing, our universe of alarms is universe - worse, since those are the facts. We want to evaluate
    # from the boosted alerts, which are real compared to v0, but witout having alarms already covered by worse
    v0_used = v0
    if use_v0_prime:
        v0_used = v0 - worse
    all = v0_used | boosted

    log.debug("Result sets sizes: Worse %d, Boosted %d, V0 %d, V0_prime %d, All %d" %
              (len(worse), len(boosted), len(v0), len(v0_used), len(all)))

    precision, recall, accuracy = _calculate_ml_scores(v0_used, boosted)

    return precision, recall, accuracy


def calculate_scores_df(
    results_folder: str,
    cleanup_base_dir="/tmp",
    worse_dir="worse",
    boost_dir="boosted",
    v0_dir="v0",
) -> pd.DataFrame:
    v0, worse, boosted = _calculate_score_sets(
        results_folder, cleanup_base_dir, worse_dir, boost_dir, v0_dir)

    v0_prime = v0-worse

    proj_to_alert_count = defaultdict(lambda: 0)
    for alert in v0_prime:
        proj = _project_name_from_hash(alert)
        proj_to_alert_count[proj] += 1

    average_tp_per_proj = 0
    # default to zero if no alerts were found
    if len(proj_to_alert_count) > 0:
        average_tp_per_proj = sum(
            proj_to_alert_count.values())/len(proj_to_alert_count)

    alerts_to_recover = len(v0_prime)
    alerts_recovered = len(v0_prime & boosted)
    spurious_alerts = len(boosted-v0_prime)

    precision, recall, accuracy = _calculate_ml_scores(v0_prime, boosted)

    row = [
        precision,
        recall,
        accuracy,
        alerts_to_recover,
        alerts_recovered,
        spurious_alerts,
        len(proj_to_alert_count),
        average_tp_per_proj,
    ]

    return pd.DataFrame([row], columns=['precision', 'recall', 'accuracy',
                                        'alerts to recover (atr)', 'alerts recovered', 'suprious alerts', 'projects with atr', 'avg atr per proj'])


def _calculate_ml_scores(v0: Set[str], boosted: Set[str]) -> Tuple[float, float, float]:
    all = v0 | boosted

    # Helper function to make a set iterable in a repeatable order. Used for generating
    # the item sets required by the score calculation functions in sklearn

    def repeatable_for_each_set(s: Set[int]) -> Iterable[int]:
        return sorted(list(s))

    all_ordered = repeatable_for_each_set(all)
    y_pred = [
        int(it in boosted)
        for it in all_ordered
    ]
    y_true = [
        int(it in v0)
        for it in all_ordered
    ]

    kwargs = {
        'zero_division': 0,
    }

    precision = precision_score(y_true, y_pred, pos_label=1, **kwargs)
    recall = recall_score(y_true, y_pred, pos_label=1, **kwargs)
    accuracy = accuracy_score(y_true, y_pred)

    return precision, recall, accuracy


def _predictions_info(path) -> Tuple[int]:
    df = pd.read_csv(path, names=["repr", "role", "score"])
    return df[df.role == "snk"].shape[0]


def calculate_many_scores(dirs) -> pd.DataFrame:
    collected_results = []
    for resdir in dirs:
        df = calculate_scores_df(resdir, cleanup_base_dir="tmp")
        sinks_predicted = _predictions_info(
            os.path.join(resdir, "averaged-results.csv"))
        df["predicted sinks"] = [sinks_predicted]
        collected_results.append(df)

    concated_results = pd.concat(collected_results)
    concated_results = pd.concat(
        [concated_results, concated_results.apply(['mean'])])
    concated_results['name'] = [
        f'fold {i}' for i in range(len(dirs))] + ['mean']

    # renames
    concated_results = concated_results.rename({
        "alerts to recover (atr)": "atr",
        "alerts recovered": "recovered",
        "suprious alerts": "spurious",
        "projects with atr": "progwithatr",
        "avg atr per proj": "avgatr",
        "predicted sinks": "predsinks"
    }, axis=1).astype({
        "atr": "int",
        "recovered": "int",
        "spurious": "int",
        "progwithatr": "int",
        "predsinks": "int"
    })

    return concated_results


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--results-dir",
        dest="results_dir",
        help="Directory where results are hosted. Must contain a `v0` and `worse` folder.",
        type=str
    )
    parser.add_argument(
        "--debug",
        help="Enable debug logging",
        type=bool,
        action=argparse.BooleanOptionalAction
    )

    args = parser.parse_args()
    if args.debug:
        import logging
        log.setLevel(logging.DEBUG)
    calculate_scores(args.results_dir)

name: Predict taint specifications
on:
  workflow_dispatch:
    inputs:
      query_name:
        description: >
          Name of the query to run (one of NoSql, Sql, Xss, TaintedPath).
        required: true
        default: "NoSql"
      training_projects:
        description: >
          A JSON array of projects to train on (specified either as LGTM.com slugs
          or as database names to be downloaded from Azure blob storage).
        required: true
        default: '["g/bitpay/bitcore"]'
      test_projects:
        description: >
          A JSON array of projects to predict new taint specifications for
          (specified as LGTM.com slugs); defaults to the same list as training_projects.
        required: false
        default: ""
      codeql_library_version:
        description: >
          Which branch of the `github/codeql` repository to get the query library from.
          Defaults to `lgtm.com`.
        required: false
        default: "lgtm.com"
      chunk_size:
        description: >
          Chunk size to use when computing embeddings.
        required: false
        default: "1"
jobs:
  record_parameters:
    name: Record workflow parameters
    runs-on: ubuntu-latest
    steps:
      - run: |
          echo "${{ toJSON(github.event) }}"

  train:
    name: Train on ${{ matrix.project }}
    runs-on: ubuntu-latest
    continue-on-error: true
    strategy:
      fail-fast: false
      matrix:
        project: ${{ fromJSON(github.event.inputs.training_projects) }}
    steps:
      - name: Check out this repo
        uses: actions/checkout@v2

      - name: Check out CodeQL standard library
        uses: actions/checkout@v2
        with:
          repository: github/codeql
          path: codeql-lib
          ref: "${{ github.event.inputs.codeql_library_version }}"

      - name: Download CodeQL
        uses: dsaltares/fetch-gh-release-asset@939be9e72e81fe7009b6112bc96abde38bf7b68f
        with:
          repo: "github/codeql-cli-binaries"
          version: "latest"
          file: "codeql-linux64.zip"
          target: "codeql.zip"

      - name: Set up CodeQL
        run: |
          unzip -q codeql.zip
          echo "$(readlink -f codeql)" >>$GITHUB_PATH
          mkdir $HOME/.config/codeql
          echo '--ram 6000' >$HOME/.config/codeql/config
          echo '--threads 2' >>$HOME/.config/codeql/config

      - name: Cache pip
        uses: actions/cache@v2
        with:
          path: ~/.cache/pip # This path is specific to Ubuntu
          # Look to see if there is a cache hit for the corresponding requirements file
          key: ${{ runner.os }}-pip-${{ hashFiles('code/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
            ${{ runner.os }}-

      - name: Install dependencies
        run: |
          sudo apt-get install coinor-cbc
          python -m pip install --upgrade pip
          pip install virtualenv
          virtualenv env
          source env/bin/activate
          pip install -r code/requirements.txt
          python -m pip install -i https://pypi.gurobi.com gurobipy

      - name: Sanitize project name
        id: sanitize-name
        run: |
          sanitized_name=$(echo ${{ matrix.project }} | sed s/[^a-zA-Z0-9_-]/_/g)
          echo "::set-output name=sanitized_name::$sanitized_name"

      - name: Train model
        env:
          ATM_BLOB_STORE_SAS_TOKEN: ${{ secrets.ATM_BLOB_STORE_SAS_TOKEN }}
        run: |
          source env/bin/activate
          ./scripts/train.sh \
            "${{ github.event.inputs.query_name }}" \
            "${{ matrix.project }}"

      - name: Upload scores
        uses: actions/upload-artifact@v2
        with:
          name: "${{ steps.sanitize-name.outputs.sanitized_name }}-scores"
          path: "scripts/results/*/*/reprScores.txt"

  combine_scores:
    name: Combine scores
    needs:
      - train
    runs-on: ubuntu-latest
    steps:
      - name: Check out this repo
        uses: actions/checkout@v2

      - name: Cache pip
        uses: actions/cache@v2
        with:
          path: ~/.cache/pip # This path is specific to Ubuntu
          # Look to see if there is a cache hit for the corresponding requirements file
          key: ${{ runner.os }}-pip-${{ hashFiles('code/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
            ${{ runner.os }}-

      - name: Install dependencies
        run: |
          sudo apt-get install coinor-cbc
          python -m pip install --upgrade pip
          pip install virtualenv
          virtualenv env
          source env/bin/activate
          pip install -r code/requirements.txt
          python -m pip install -i https://pypi.gurobi.com gurobipy

      - name: Download scores for individual projects
        uses: actions/download-artifact@v2

      - name: Combine scores
        run: |
          source env/bin/activate
          case "${{ github.event.inputs.query_name }}" in
            "NoSql")
              lib="NosqlInjection"
            ;;
            "Sql")
              lib="SqlInjection"
            ;;
            "Xss")
              lib="DomBasedXss"
            ;;
            "TaintedPath")
              lib="TaintedPath"
            ;;
            *)
              echo "Unsupported query $query_name; should be NoSql, Sql, Xss or TaintedPath."
              exit 1
            ;;
          esac
          find . -name reprScores.txt
          python code/misc/combinescores.py \
            --query-name "${lib}Worse" \
            --project-dir "*-scores"

      - name: Upload scores
        uses: actions/upload-artifact@v2
        with:
          name: scores
          path: "allscores_*.txt"

  predict:
    name: Predict new sinks for ${{ matrix.project }}
    needs:
      - combine_scores
    runs-on: ubuntu-latest
    continue-on-error: true
    strategy:
      fail-fast: false
      matrix:
        project: ${{ fromJSON(github.event.inputs.test_projects || github.event.inputs.training_projects) }}
    steps:
      - name: Check out this repo
        uses: actions/checkout@v2

      - name: Check out CodeQL standard library
        uses: actions/checkout@v2
        with:
          repository: github/codeql
          path: codeql-lib
          ref: "${{ github.event.inputs.codeql_library_version }}"

      - name: Download CodeQL
        uses: dsaltares/fetch-gh-release-asset@939be9e72e81fe7009b6112bc96abde38bf7b68f
        with:
          repo: "github/codeql-cli-binaries"
          version: "latest"
          file: "codeql-linux64.zip"
          target: "codeql.zip"

      - name: Set up CodeQL
        run: |
          unzip -q codeql.zip
          echo "$(readlink -f codeql)" >>$GITHUB_PATH
          mkdir $HOME/.config/codeql
          echo '--ram 6000' >$HOME/.config/codeql/config
          echo '--threads 2' >>$HOME/.config/codeql/config

      - name: Cache pip
        uses: actions/cache@v2
        with:
          path: ~/.cache/pip # This path is specific to Ubuntu
          # Look to see if there is a cache hit for the corresponding requirements file
          key: ${{ runner.os }}-pip-${{ hashFiles('code/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
            ${{ runner.os }}-

      - name: Install dependencies
        run: |
          sudo apt-get install coinor-cbc
          python -m pip install --upgrade pip
          pip install virtualenv
          virtualenv env
          source env/bin/activate
          pip install -r code/requirements.txt
          python -m pip install -i https://pypi.gurobi.com gurobipy
          cd triager
          npm i
          cd ..

      - name: Download combined scores
        uses: actions/download-artifact@v2
        with:
          name: scores

      - name: Sanitize project name
        id: sanitize-name
        run: |
          sanitized_name=$(echo ${{ matrix.project }} | sed s/[^a-zA-Z0-9_-]/_/g)
          echo "::set-output name=sanitized_name::$sanitized_name"

      - name: Predict new sinks
        env:
          ATM_BLOB_STORE_SAS_TOKEN: ${{ secrets.ATM_BLOB_STORE_SAS_TOKEN }}
        run: |
          source env/bin/activate
          n=$(compgen -G "allscores_*.txt" | wc -l)
          if [ $n -ne 1 ]; then
            echo "Expected a single scores file; got $n."
            exit 1
          fi
          ./scripts/predict.sh \
            $(compgen -G "allscores_*.txt") \
            "${{ matrix.project }}" \
            "${{ steps.sanitize-name.outputs.sanitized_name }}-predictions.json" \
            ${{ github.event.inputs.query_name }}

      - name: Upload predictions
        uses: actions/upload-artifact@v2
        with:
          name: "${{ steps.sanitize-name.outputs.sanitized_name }}-predictions"
          path: "${{ steps.sanitize-name.outputs.sanitized_name }}-predictions.json"

  create_embeddings:
    name: Compute embeddings of sink candidates for ${{ matrix.project }}
    needs:
      - predict
    runs-on: ubuntu-latest
    continue-on-error: true
    strategy:
      fail-fast: false
      matrix:
        project: ${{ fromJSON(github.event.inputs.test_projects || github.event.inputs.training_projects) }}
    steps:
      - name: Check out this repo
        uses: actions/checkout@v2

      - name: Sanitize project name
        id: sanitize-name
        run: |
          sanitized_name=$(echo ${{ matrix.project }} | sed s/[^a-zA-Z0-9_-]/_/g)
          echo "::set-output name=sanitized_name::$sanitized_name"

      - name: Download predictions
        uses: actions/download-artifact@v2
        with:
          name: "${{ steps.sanitize-name.outputs.sanitized_name }}-predictions"
          path: predictions

      - name: Fetch database
        env:
          ATM_BLOB_STORE_SAS_TOKEN: ${{ secrets.ATM_BLOB_STORE_SAS_TOKEN }}
        run: |
          ./scripts/fetch_database.py databases ${{ matrix.project }} ${{ github.event.inputs.query_name }}

      - name: Compute embeddings
        run: |
          python -m pip install --upgrade pip
          pip install -r similarity-api/requirements.txt
          ./similarity-api/createEmbeddings.py \
            --predictions "predictions/${{ steps.sanitize-name.outputs.sanitized_name }}-predictions.json" \
            --databases databases \
            --output embeddings \
            --chunk-size ${{ github.event.inputs.chunk_size }}

      - name: Upload embeddings
        uses: actions/upload-artifact@v2
        with:
          name: "${{ steps.sanitize-name.outputs.sanitized_name }}-embeddings"
          path: "embeddings/*.pickle"

  update_predictions:
    name: Update the scores of sink candidates for ${{ matrix.project }}
    needs:
      - create_embeddings
    runs-on: ubuntu-latest
    continue-on-error: true
    strategy:
      fail-fast: false
      matrix:
        project: ${{ fromJSON(github.event.inputs.test_projects || github.event.inputs.training_projects) }}
    steps:
      - name: Check out this repo
        uses: actions/checkout@v2
        with:
          lfs: true

      - name: Sanitize project name
        id: sanitize-name
        run: |
          sanitized_name=$(echo ${{ matrix.project }} | sed s/[^a-zA-Z0-9_-]/_/g)
          echo "::set-output name=sanitized_name::$sanitized_name"

      - name: Download predictions
        uses: actions/download-artifact@v2
        with:
          name: "${{ steps.sanitize-name.outputs.sanitized_name }}-predictions"
          path: predictions

      - name: Download embeddings
        uses: actions/download-artifact@v2
        with:
          name: "${{ steps.sanitize-name.outputs.sanitized_name }}-embeddings"
          path: "embeddings/"
      
      - name: Extract ks embeddings files
        run: | 
            unzip ./similarity-api/ksEmbs/EmbKnownStm.zip -d ./similarity-api/ksEmbs/ 
            unzip ./similarity-api/ksEmbs/EmbKnownFunc.zip -d ./similarity-api/ksEmbs/ 

      - name: Update scores
        run: |
          python -m pip install --upgrade pip
          pip install -r similarity-api/requirements.txt
          python ./similarity-api/updatePredictions.py \
            --predictions "predictions/${{ steps.sanitize-name.outputs.sanitized_name }}-predictions.json" \
            --embeddings "embeddings/" \
            --chunk-size ${{ github.event.inputs.chunk_size }} \
            --chunk-size-ks 50

      - name: Upload predictions
        uses: actions/upload-artifact@v2
        with:
          name: "${{ steps.sanitize-name.outputs.sanitized_name }}-predictions"
          path: "predictions/${{ steps.sanitize-name.outputs.sanitized_name }}-predictions.json"

  combine_embeddings:
    name: Combine all predictions and their embeddings
    needs:
      - update_predictions
    runs-on: ubuntu-latest
    steps:
      - name: Download predictions and embeddings
        uses: actions/download-artifact@v2

      - name: Combine predictions
        run: |
          mkdir predictions
          jq -s flatten *-predictions/*.json >predictions/predictions.json

      - name: Put all of them into a single directory
        run: |
          mkdir _results
          cp -r *-embeddings _results || true
          cp -r *-predictions _results || true
          cp predictions/predictions.json _results || true

      - name: Upload predictions
        uses: actions/upload-artifact@v2
        with:
          name: predictions
          path: predictions

      - name: Upload predictions and embeddings
        uses: actions/upload-artifact@v2
        with:
          # important: the name must be lexicographically small, since the CLI can only download
          # the first 100 artifacts in lexiographic order
          name: "_results"
          path: "_results"

  build_report:
    name: Build report for triaging new sinks
    needs:
      - combine_embeddings
    runs-on: ubuntu-latest
    steps:
      - name: Check out this repo
        uses: actions/checkout@v2

      - name: Download predictions for individual projects
        uses: actions/download-artifact@v2

      - name: Copy predictions to triager
        run: |
          cp predictions/predictions.json triager/data

      - name: Build report
        run: |
          cd triager
          echo 'NEXT_PUBLIC_SIMILARITY_SERVER=' >.env.local
          npm install
          npm run build
          npm run export
          touch out/.nojekyll

      - name: Deploy report to gh-pages
        uses: JamesIves/github-pages-deploy-action@4.1.1
        with:
          branch: gh-pages
          folder: triager/out

      - name: Finish
        run: |
          echo 'Triaging dashboard deployed to https://github.github.io/ml-ql-taint-specification-mining'